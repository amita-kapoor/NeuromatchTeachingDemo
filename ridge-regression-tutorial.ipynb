{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge-regression (Regression + regularization) mini-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "In this set of exercises we will try to predict the neural activity from videos of animal behavior using ridge-regression.  The data we will use come from a larger dataset published here: https://science.sciencemag.org/content/364/6437/eaav7893.  For the data here, the mouse was not doing a task, and was free to run and whisk and groom as it pleased. In the meantime there was random noise shown on the screen in front of the mouse.\n",
    "\n",
    "The neural data consists of the activity of excitatory neurons in visual cortex recorded using two-photon calcium imaging. For these exercises it has been pre-processed.  There were originally 5439 neurons, but here we have reduced that to the top 256 principal components sampled at 1Hz.\n",
    "\n",
    "The behavioral data was recorded for the same period. While the mouse was on the microscope, we captured its face on video and took the top 500 PCs of the motion energy of this video data, sampled at 1Hz. \n",
    "\n",
    "This particular group of neurons have been classified as NOT stimulus-selective, so we are interested in understanding what they are doing. The motivation for this regression analysis is to investigate whether the neural activity is explained by mouse behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0) Set up python packages and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell imports all the functions and data we need.\n",
    "import os # os stands for \"operating system\" and includes read/write routines etc. \n",
    "import numpy as np # by far the most used library for everyday computation\n",
    "from scipy import io # this is for importing Matlab data files\n",
    "from scipy import stats # here we import a whole sub-library of stats functions\n",
    "from scipy.ndimage import gaussian_filter # here we import a single function for gaussian_filtering\n",
    "from sklearn.decomposition import PCA # check out all the other dimensionality reduction methods in \"decomposition\"\n",
    "from matplotlib import pyplot as plt # all of our plotting is done with plt\n",
    "from scipy.stats import zscore\n",
    "import urllib\n",
    "%matplotlib inline \n",
    "\n",
    "# download 2-photon data\n",
    "\n",
    "with urllib.request.urlopen('https://github.com/kbonnen/NeuromatchTeachingDemo/raw/master/teaching-demo-data.npy') as response:\n",
    "    with open('teaching-demo-data.npy','wb') as f:\n",
    "        f.write(response.read())\n",
    "\n",
    "data = np.load('teaching-demo-data.npy',allow_pickle=True).item()\n",
    "\n",
    "# Extract the data into variables\n",
    "neural_reduced = zscore(data['neural_reduced'], axis=1) # 2-photon neural activity with mean 0 and standard deviation, collected at 3Hz\n",
    "behavior_reduced = data['behavior_reduced'] # video of the mouse's face during behavior projected onto the top 500 principal components\n",
    "del data\n",
    "\n",
    "\n",
    "# Examine the data a bit:\n",
    "print('neural data: ' + str(neural_reduced.shape[0]) + ' neural components x ' + str(neural_reduced.shape[1]) + ' time points (seconds)')\n",
    "print('behavioral data: ' + str(behavior_reduced.shape[0]) + ' behavioral components x ' + str(behavior_reduced.shape[1]) + ' time points (seconds)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Prediction of Neural principal components using Behavioral principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "# *** use interleaved segments ***\n",
    "nsegs = 20\n",
    "nt = neural_reduced.shape[1]\n",
    "nlen  = nt/nsegs\n",
    "ninds = np.linspace(0,nt-nlen,nsegs).astype(int)\n",
    "itest = (ninds[:,np.newaxis] + np.arange(0,nlen*0.25,1,int)).flatten()\n",
    "itrain = np.ones(nt, np.bool)\n",
    "itrain[itest] = 0\n",
    "\n",
    "fig=plt.figure(figsize=(8,4))\n",
    "ax = fig.add_axes([0,.05,.4,.8])\n",
    "ax.plot(itrain)\n",
    "ax.set_title(\"training set (boolean)\")\n",
    "\n",
    "ax = fig.add_axes([0.5,.05,.4,.8])\n",
    "ax.plot(itest)\n",
    "ax.set_title(\"testing set (indices)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What is the motivation for splitting the data into training and test sets by segments in the manner illustrated above? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Linear Regression\n",
    "\n",
    "Now use linear regression to perform the prediction, predict $Y$ using $X$:\n",
    "\n",
    "$$ A = (X_\\text{train}X_\\text{train}^\\top)^{-1} (X_\\text{train} Y_\\text{train}^\\top)$$\n",
    "\n",
    "where $X$ is the reduced behavioral data and $Y$ is the reduced neural data\n",
    "\n",
    "Then the prediction is:\n",
    "\n",
    "$$ \\hat Y_\\text{test} = A^\\top X_\\text{test} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XXt = ?  \n",
    "XYt = ? \n",
    "A = np.linalg.solve(XXt, XYt )\n",
    "\n",
    "Yhat_linear = ?\n",
    "Yhat_train_linear = ?\n",
    "\n",
    "# Calculate the proportion of variance explained.\n",
    "varexp_linear = 1 - ((Yhat_linear - neural_reduced[:,itest])**2).sum(axis=1)/(neural_reduced[:,itest]**2).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of principal component activity and variance explained\n",
    "\n",
    "fig=plt.figure(figsize=(12,3))\n",
    "\n",
    "ipc = 0 ### which neural PC to plot\n",
    "\n",
    "ax = fig.add_axes([0.05,.05,.75,.95])\n",
    "ax.plot(neural_reduced[ipc,itest],color=[0,0,0])\n",
    "ax.plot(Yhat_linear[ipc], color=[0,.5,0])\n",
    "ax.set_title('PC %d'%ipc)\n",
    "ax.set_xlabel('time')\n",
    "ax.set_ylabel('activity')\n",
    "\n",
    "ax = fig.add_axes([0.9,.05, .2, .8])\n",
    "ax.scatter(ipc+1, varexp_linear[ipc],marker='x',color='r',s=200, lw=4, zorder=10)\n",
    "ax.semilogx(np.arange(1,varexp_linear.size+1), varexp_linear, color=[0,.5,0])\n",
    "ax.legend(['linear','PC'])\n",
    "ax.set_xlabel('PC')\n",
    "ax.set_ylabel('fraction variance explained')\n",
    "ax.set_title('PC %d, varexp=%0.2f'%(ipc,varexp_linear[ipc]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Ridge Regression\n",
    "\n",
    "Now let's regularize the linear regression using the L2-norm (a.k.a. ridge regression):\n",
    "\n",
    "$$ A = (X_\\text{train}X_\\text{train}^\\top + \\lambda I)^{-1} (X_\\text{train} Y_\\text{train}^\\top)$$\n",
    "\n",
    "Then the prediction is still:\n",
    "\n",
    "$$ \\hat Y_\\text{test} = A^\\top X_\\text{test} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XXt = ?\n",
    "lam = ? # regularizer\n",
    "XXt += lam*?\n",
    "XYt = np.matmul(behavior_reduced[:,itrain], neural_reduced[:,itrain].T)  \n",
    "A = np.linalg.solve(XXt, XYt )\n",
    "\n",
    "Yhat_ridge = ?\n",
    "Yhat_train_ridge = ?\n",
    "\n",
    "# Calculate the proportion of variance explained.\n",
    "varexp_ridge = 1 - ((Yhat_ridge - neural_reduced[:,itest])**2).sum(axis=1)/(neural_reduced[:,itest]**2).sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of principal component activity and variance explained\n",
    "\n",
    "fig=plt.figure(figsize=(12,3))\n",
    "\n",
    "ipc = 0 ### which PC to plot\n",
    "\n",
    "ax = fig.add_axes([0.05,.05,.75,.95])\n",
    "ax.plot(neural_reduced[ipc,itest],color=[0,0,0])\n",
    "ax.plot(Yhat_ridge[ipc], color=[1,.5,0])\n",
    "ax.set_title('PC %d'%ipc)\n",
    "ax.set_xlabel('time')\n",
    "ax.set_ylabel('activity')\n",
    "\n",
    "ax = fig.add_axes([0.9,.05, .2, .8])\n",
    "ax.scatter(ipc+1, varexp_ridge[ipc],marker='x',color='r',s=200, lw=4, zorder=10)\n",
    "ax.semilogx(np.arange(1,varexp_ridge.size+1), varexp_ridge, color=[1,.5,0])\n",
    "ax.legend(['ridge','PC'])\n",
    "ax.set_xlabel('PC')\n",
    "ax.set_ylabel('fraction variance explained')\n",
    "ax.set_title('PC %d, varexp=%0.2f'%(ipc,varexp_ridge[ipc]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Compare Linear Regression to Ridge Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of principal component activity from the training set and variance explained\n",
    "\n",
    "fig=plt.figure(figsize=(12,3))\n",
    "\n",
    "ipc = 0 ### which PC to plot\n",
    "\n",
    "# Plot Yhat for the training set for ridge regression\n",
    "ax = fig.add_axes([0.05,.55,.75,.45])\n",
    "ax.plot(neural_reduced[ipc,itrain],color=[0,0,0])\n",
    "ax.plot(?[ipc], color=[1,.5,0])\n",
    "ax.set_title('PC %d'%ipc)\n",
    "\n",
    "# Plot Yhat for the training set for linear regression\n",
    "ax = fig.add_axes([0.05,0,.75,.45])\n",
    "ax.plot(neural_reduced[ipc,itrain],color=[0,0,0])\n",
    "ax.plot(?[ipc],  color=[0.,.5,0])\n",
    "ax.set_xlabel('time')\n",
    "ax.set_ylabel('activity')\n",
    "\n",
    "# Plot the variance explained\n",
    "ax = fig.add_axes([0.9,.05, .2, .8])\n",
    "ax.scatter(ipc+1, varexp_ridge[ipc],marker='x',color='r',s=200, lw=4, zorder=10)\n",
    "ax.semilogx(np.arange(1,varexp_ridge.size+1), varexp_ridge, color=[1,.5,0])\n",
    "ax.semilogx(np.arange(1,varexp_linear.size+1), varexp_linear, color=[0.,.5,0])\n",
    "ax.set_xlabel('PC')\n",
    "ax.set_ylabel('fraction variance explained')\n",
    "ax.set_title('PC %d, varexp=%0.2f'%(ipc,varexp_ridge[ipc]))\n",
    "ax.legend(['ridge','linear','PC'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Do you see evidence of overfitting for the linear solution? Where?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
